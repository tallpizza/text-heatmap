# Haiku 기반 중요도 추출 설계

## 개요

기존 DistilBERT self-attention 방식을 Claude Haiku API 기반 추출 요약 방식으로 교체.

### 변경 이유
- Self-attention은 "중요도"와 항상 일치하지 않음
- Haiku는 의미 기반으로 진짜 중요한 부분을 추출 가능
- 저렴하고 빠름 ($0.25/1M 입력 토큰)
- 로컬 모델 관리 불필요

## 전체 흐름

```
사용자 입력 (텍스트)
       ↓
  백엔드 API (/api/analyze)
       ↓
  Haiku API 호출
  "중요한 키워드/구문 추출해줘"
       ↓
  응답: [{"text": "재고 스냅샷", "score": 0.9}, ...]
       ↓
  원문에서 키워드 위치 매칭
       ↓
  단어별 점수 배열 생성 (words, scores)
       ↓
  프론트엔드 (기존 히트맵 뷰 그대로)
```

## Haiku 프롬프트

```python
prompt = """
다음 텍스트에서 독자가 빠르게 핵심을 파악하는 데 도움이 되는
중요한 키워드와 구문을 추출해주세요.

규칙:
- 텍스트 길이에 비례해서 적절한 수만큼 추출
- 각 항목에 중요도 점수 (0.0~1.0) 부여
- 고유명사, 숫자, 핵심 동사, 핵심 개념 위주

점수 기준:
- 0.9~1.0: 핵심 주제, 결론, 액션 아이템
- 0.7~0.8: 주요 개념, 중요 조건
- 0.5~0.6: 부가 정보지만 알아두면 좋은 것

JSON 형식으로만 응답 (다른 텍스트 없이):
{"keywords": [{"text": "키워드", "score": 0.9}, ...]}

텍스트:
{text}
"""
```

## 키워드 → 단어 매칭

### 입력
```python
keywords = [
    {"text": "재고 스냅샷", "score": 0.9},
    {"text": "긴급 발주", "score": 0.8},
]
words = ["재고", "스냅샷", "데이터의", "최신", ...]
```

### 출력
```python
scores = [0.9, 0.9, 0.0, 0.0, ...]
```

### 매칭 규칙
1. 키워드를 공백으로 분리하여 단어 시퀀스로 변환
2. 원문에서 해당 시퀀스 위치 찾기
3. 같은 단어 여러 번 등장 → 모든 위치에 점수
4. 여러 키워드에 포함 → 최고 점수 사용

## 코드 구조

```
backend/
├── services/
│   ├── extraction.py     # Haiku 호출 + 매칭 (새로 추가)
│   ├── attention.py      # 삭제
│   └── model_loader.py   # 삭제
├── config.py             # ANTHROPIC_API_KEY 추가
└── routers/
    └── analyze.py        # extraction.py 사용하도록 변경
```

## 환경변수

```env
ANTHROPIC_API_KEY=sk-ant-...
MODEL_NAME=claude-3-5-haiku-latest
MAX_CHARACTERS=100000
```

## 청킹

- Haiku 컨텍스트: 200K 토큰
- MAX_CHARACTERS 100,000자 ≈ 30~50K 토큰
- **청킹 불필요**

## 에러 처리

1. API 키 없음 → 서버 시작 시 경고
2. API 호출 실패 → 500 에러 + 메시지
3. JSON 파싱 실패 → 재시도 또는 빈 결과

## 비용 예상

- 평균 텍스트 5,000자 ≈ 1,500 토큰
- 1회 요청: ~$0.0004
- 1,000회 요청: ~$0.40
